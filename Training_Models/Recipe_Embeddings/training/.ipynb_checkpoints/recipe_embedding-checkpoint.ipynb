{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kratikakothari/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kratikakothari/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### kaggle and nature dataset(to be used for predicting ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the dataset\n",
    "# The dataset is kaggle and nature dataset\n",
    "# It contains set of ingredients and the associated cuisine for each recipe.\n",
    "f_kaggleNature = open('kaggle_and_nature.csv', newline = '')    \n",
    "csv_reader = csv.reader(f_kaggleNature, delimiter='\\t')\n",
    "\n",
    "id_ingredients_cuisine = []\n",
    "cuisines = []\n",
    "i = 0\n",
    "for row in csv_reader:\n",
    "    temp = dict()\n",
    "    temp['id'] = i\n",
    "    ingredients = []\n",
    "    for ingredient in row[0].split(\",\")[1:]:\n",
    "        ingredients.append(ingredient.replace(\"_\",\" \"))\n",
    "    temp['ingredients'] = ingredients\n",
    "    temp['cuisine'] = row[0].split(\",\")[0]\n",
    "    id_ingredients_cuisine.append(temp)\n",
    "    cuisines.append(row[0].split(\",\")[0])\n",
    "    i = i + 1\n",
    "id_ingredients_cuisine\n",
    "# Removing Punctuation, Stopwords\n",
    "# Lemmatization\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "intab = '''!()-[]{};:'\"\\,<>?@#$%^&*_~'''\n",
    "outtab = \"_\" * len(intab)\n",
    "trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "new_id_ingredients_cuisine = []\n",
    "max_ingredients = 0\n",
    "\n",
    "for recipe_id,recipe in enumerate(id_ingredients_cuisine):\n",
    "    temp_ingredients = []\n",
    "    for ingredient in recipe[\"ingredients\"]:\n",
    "        word_tokens = word_tokenize(ingredient)\n",
    "        l = []\n",
    "        for word in word_tokens:\n",
    "            if word not in stop_words:\n",
    "                word = word.translate(trantab).replace(\"_\", \"\").lower()\n",
    "                l.append(lemmatizer.lemmatize(word))\n",
    "        ingredient_modified = \" \".join(l)\n",
    "        temp_ingredients.append(ingredient_modified)\n",
    "    temp = dict()\n",
    "    temp['id'] = recipe_id\n",
    "    temp['ingredients'] = sorted(temp_ingredients)\n",
    "    temp['cuisine'] = recipe[\"cuisine\"]\n",
    "    new_id_ingredients_cuisine.append(temp)\n",
    "    if(len(temp_ingredients) > max_ingredients):\n",
    "        max_ingredients  = len(temp_ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### building corpus for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building corpus from kaggle and nature dataset\n",
    "corpus = []\n",
    "for recipe_id,recipe in enumerate(new_id_ingredients_cuisine):\n",
    "    corpus.append(recipe[\"ingredients\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kaggle&nature_corpus.pkl','wb') as f:\n",
    "    pickle.dump(corpus,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recipe_vectors():\n",
    "#     making a new dictionary with key as recipe id \n",
    "#     new_id_ingredients_cuisine\n",
    "    model = Word2Vec.load('word2vec_skipgram_kg_ng7.model')\n",
    "    id_vector = dict()\n",
    "    id_ingredients = dict()\n",
    "    id_cuisine = dict()\n",
    "    for recipe_id,recipe in enumerate(new_id_ingredients_cuisine):\n",
    "        current_sum = np.zeros(100)\n",
    "        n = len(recipe['ingredients'])\n",
    "        for ingredient in recipe['ingredients']:\n",
    "            vector = model.wv[ingredient]\n",
    "            current_sum = np.add(current_sum,vector)\n",
    "        current_sum = current_sum/n\n",
    "        id_vector[recipe_id] = current_sum\n",
    "        id_ingredients[recipe_id] = recipe['ingredients']\n",
    "        id_cuisine[recipe_id] = recipe['cuisine']\n",
    "    with open('kaggle&nature_Id_vectors.pkl','wb') as f:\n",
    "        pickle.dump(id_vector,f)\n",
    "    with open('kaggle&nature_Id_ingredients.pkl','wb') as g:\n",
    "        pickle.dump(id_ingredients,g)\n",
    "    with open('kaggle&nature_Id_cuisine.pkl','wb') as h:\n",
    "        pickle.dump(id_cuisine,h)\n",
    "    return id_vector,id_ingredients,id_cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CulinaryDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building corpus from culinaryDB\n",
    "import pickle\n",
    "with open('culinaryDB_new_recipes.pkl', 'rb') as f:\n",
    "    id_ingredients = pickle.load(f)\n",
    "corpus = []\n",
    "max_ingredients = 0\n",
    "for recipe_id in id_ingredients:\n",
    "    corpus.append(id_ingredients[recipe_id])\n",
    "    if len(id_ingredients[recipe_id]) > max_ingredients:\n",
    "        max_ingredients = len(id_ingredients[recipe_id]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('culinarDB_new_corpus.pkl','wb') as f:\n",
    "    pickle.dump(corpus,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recipe_vectors_for_culinaryDB():\n",
    "#     making a new dictionary with key as recipe id \n",
    "    id_vector = dict()\n",
    "    id_\n",
    "    for recipe_id in id_ingredients:\n",
    "        current_sum = np.zeros(100)\n",
    "        n = len(id_ingredients[recipe_id])\n",
    "        for ingredient in id_ingredients[recipe_id]:\n",
    "            vector = model.wv[ingredient]\n",
    "            current_sum = np.add(current_sum,vector)\n",
    "        current_sum = current_sum/n\n",
    "        id_vector[recipe_id] = current_sum\n",
    "    return id_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KAGGLE2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building corpus from kaggle2 dataset\n",
    "data = pd.read_csv('RAW_recipes.csv')\n",
    "recipe_id = data['id']\n",
    "recipe_names = data['name']\n",
    "ingredients = data['ingredients']\n",
    "id_recipeName = pd.concat([recipe_id, recipe_names], axis=1)\n",
    "id_ingredients = pd.concat([recipe_id, ingredients], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building corpus from kaggle2 dataset\n",
    "corpus = []\n",
    "max_ingredients = 0\n",
    "for i in ingredients:\n",
    "    corpus.append(i)\n",
    "    if len(i) > max_ingredients:\n",
    "        max_ingredients = len(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "intab = '''!()-[]{};:'\"\\,<>?@#$%^&*_~'''\n",
    "outtab = \"_\" * len(intab)\n",
    "trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "new_ingredients = []\n",
    "max_ingredients = 0\n",
    "\n",
    "for recipe in ingredients:\n",
    "    temp_ingredients = []\n",
    "    l = recipe.split(\",\")\n",
    "    for ingredient in l:\n",
    "        word_tokens = word_tokenize(ingredient)\n",
    "        l = []\n",
    "        for word in word_tokens:\n",
    "            if word not in stop_words:\n",
    "                word = word.translate(trantab).replace(\"_\", \"\").lower()\n",
    "                l.append(lemmatizer.lemmatize(word))\n",
    "        ingredient_modified = \" \".join(l)\n",
    "        temp_ingredients.append(ingredient_modified.strip())\n",
    "    new_ingredients.append(temp_ingredients)\n",
    "    if(len(temp_ingredients)>max_ingredients):\n",
    "        max_ingredients = len(temp_ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recipe_vectors_for_kaggle2():\n",
    "#     making a new dictionary with key as recipe id \n",
    "    id_vector = dict()\n",
    "    id_ingredients = dict()\n",
    "    for i in range(len(new_ingredients)):\n",
    "        current_sum = np.zeros(100)\n",
    "        n = len(new_ingredients[i])\n",
    "        for ingredient in new_ingredients[i]:\n",
    "            vector = model.wv[ingredient]\n",
    "            current_sum = np.add(current_sum,vector)\n",
    "        current_sum = current_sum/n\n",
    "        id_vector[recipe_id[i]] = current_sum\n",
    "        id_ingredients[recipe_id[i]] = new_ingredients[i]\n",
    "    return id_vector,id_ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_names = dict()\n",
    "for i in range(len(recipe_id)):\n",
    "    id_names[recipe_id[i]] = recipe_names[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('kaggle2_Id_vectors.pkl','wb') as f:\n",
    "    pickle.dump(id_vector,f)\n",
    "with open('kaggle2_Id_ingredients.pkl','wb') as g:\n",
    "    pickle.dump(id_ingredients,g)\n",
    "with open('kaggle2_Id_names.pkl','wb') as h:\n",
    "    pickle.dump(id_names,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training word2vec model\n",
    "with open('culinarDB_new_corpus.pkl','rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "# corpus = new_ingredients\n",
    "embedding_size = 100\n",
    "no_of_workers = 2 # better to have as many workers an number of cores on the machine\n",
    "window_size = max_ingredients \n",
    "sg_ = 1 # 0 for CBOW and 1 for skip-gram\n",
    "no_of_negative_samples = 7 \n",
    "        \n",
    "model = Word2Vec(corpus, min_count = 1, size = embedding_size, workers = no_of_workers, window = window_size, sg = sg_,negative = no_of_negative_samples)\n",
    "\n",
    "model.save(\"word2vec_cl_new_ng7.model\")\n",
    "# model.wv.save_word2vec_format('word2vec_skipgram_kg_ng7.txt', binary=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
